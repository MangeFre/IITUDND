{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from utilities import data_handler\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer # for tokenization only\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE CELL BELLOW CONTAINS THE FILENAMES TO CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "UNLABLED_DATA = '/Users/ianmagnusson/IITUDND/data/retrieved_data/tweets/maria_extras.json'\n",
    "LABLED_DATA = '/Users/ianmagnusson/IITUDND/data/CrisisMMD_v1.0/json/hurricane_maria_final_data.json'\n",
    "CLASS_DATA = '/Users/ianmagnusson/IITUDND/data/CrisisMMD_v1.0/annotations/hurricane_maria_final_data.tsv'\n",
    "NPY_OUTPUT_DIR = '/Users/ianmagnusson/IITUDND/data/extracted_features/combined_NLP/maria/kfold/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# get tweets from file and split into test/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data_handler.DataHandler(UNLABLED_DATA,LABLED_DATA,CLASS_DATA)\n",
    "\n",
    "_, validation_sets = data.get_k_fold_split(10)\n",
    "\n",
    " #test_labeled, test_histories, test_histories_by_target, train_merged, train_classes, test_classes = data.get_train_test_split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "conversion_file = '../models/gensim_glove.txt'\n",
    "# convert glove format to work with gensim. tutorial here https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
    "# _ = glove2word2vec('/Users/ianmagnusson/IITUDND/models/glove.twitter.27B.200d.txt', conversion_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load model, NOTE this is very slow!\n",
    "glove = KeyedVectors.load_word2vec_format(conversion_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CODE FOR EXTRACTIONS\n",
    "\n",
    "EMBED_DIM = 200\n",
    "\n",
    "def embed_tweets(tweet_jsons):\n",
    "    X_embedded = np.zeros((len(tweet_jsons),EMBED_DIM))\n",
    "    tokenizer = CountVectorizer().build_tokenizer()\n",
    "    for i, tweet_json in enumerate(tweet_jsons):\n",
    "        text = tweet_json['text'].lower()\n",
    "        tokens = [token for token in tokenizer(text) if token not in ENGLISH_STOP_WORDS]\n",
    "        num_in_vocab = 0\n",
    "        for token in tokens:\n",
    "            if token in glove:\n",
    "                X_embedded[i] += glove[token]\n",
    "                num_in_vocab += 1\n",
    "        X_embedded[i] = X_embedded[i] / num_in_vocab\n",
    "    return X_embedded\n",
    "\n",
    "def embed_histories(histories):\n",
    "    X_embedded = np.zeros((len(histories),EMBED_DIM))\n",
    "    tokenizer = CountVectorizer().build_tokenizer()\n",
    "    for i, history in enumerate(histories):\n",
    "        text = ' '.join([tweet_json['text'].lower() for tweet_json in history])\n",
    "        tokens = [token for token in tokenizer(text) if token not in ENGLISH_STOP_WORDS]\n",
    "        num_in_vocab = 0\n",
    "        for token in tokens:\n",
    "            if token in glove:\n",
    "                X_embedded[i] += glove[token]\n",
    "                num_in_vocab += 1\n",
    "        X_embedded[i] = X_embedded[i] / num_in_vocab\n",
    "    return X_embedded\n",
    "\n",
    "def proccess_seq(histories_by_target):\n",
    "    X_seq = [] # a list of 2d tensors of shape (len(seq), embed_dim)\n",
    "    for history in histories_by_target:\n",
    "        X_seq.append(embed_tweets(history))\n",
    "    \n",
    "    return X_seq\n",
    "\n",
    "SVD_COMPONENTS = 200\n",
    "\n",
    "def construct_vectorizer_and_SVD(merged):\n",
    "    allTweets = []\n",
    "    for i, tweet_json in enumerate(merged):\n",
    "        text = tweet_json['text'].lower()\n",
    "        allTweets.append(text)\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_merged = vectorizer.fit_transform(allTweets)\n",
    "    svd = TruncatedSVD(n_components=SVD_COMPONENTS, n_iter=7, random_state=42)\n",
    "    svd.fit(tfidf_merged)\n",
    "    return vectorizer, svd\n",
    "\n",
    "\n",
    "def vectorize_histories(histories, vectorizer, svd):\n",
    "    rawHistories = [] # Will be in order\n",
    "    for i, history in enumerate(histories):\n",
    "        text = ' '.join([tweet_json['text'].lower() for tweet_json in history])\n",
    "        rawHistories.append(text)\n",
    "    histArr = vectorizer.transform(rawHistories)\n",
    "    histFeatureArr = svd.transform(histArr)\n",
    "    return histFeatureArr\n",
    "\n",
    "\n",
    "def vectorize_tweets(tweets, vectorizer, svd):\n",
    "    labeledTweets = []  # Will be in order\n",
    "    for i, tweet_json in enumerate(tweets):\n",
    "        text = tweet_json['text'].lower()\n",
    "        labeledTweets.append(text)\n",
    "    tweetArr = vectorizer.transform(labeledTweets)\n",
    "    tweetFeatureArr = svd.transform(tweetArr)\n",
    "    return tweetFeatureArr\n",
    "\n",
    "def proccess_seq_tfidf(histories_by_target, vectorizer, svd):\n",
    "    X_seq = [] # a list of 2d tensors of shape (len(seq), SVD_COMPONENTS)\n",
    "    for history in histories_by_target:\n",
    "        X_seq.append(vectorize_tweets(history, vectorizer, svd))\n",
    "    return X_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i, val_set in enumerate(validation_sets):\n",
    "    print(i)\n",
    "    \n",
    "    out_dir = NPY_OUTPUT_DIR + str(i) + '/'\n",
    "    train_labeled, train_histories, train_histories_by_target, train_classes, train_merged = val_set[0]\n",
    "    test_labeled, test_histories, test_histories_by_target, test_classes, _ = val_set[1]\n",
    "    \n",
    "    # get data\n",
    "    # classes:\n",
    "    y_train = np.array(train_classes)\n",
    "    y_test = np.array(test_classes)\n",
    "    \n",
    "    # glove baseline: \n",
    "    X_labeled_train = embed_tweets(train_labeled)\n",
    "    X_histories_train = embed_histories(train_histories)\n",
    "    X_labeled_test = embed_tweets(test_labeled)\n",
    "    X_histories_test = embed_histories(test_histories)\n",
    "    \n",
    "    # glove sequence features\n",
    "    X_seq_train = proccess_seq(train_histories_by_target)\n",
    "    X_seq_test = proccess_seq(test_histories_by_target)\n",
    "    \n",
    "    # tfidf baseline\n",
    "    vectorizer, svd = construct_vectorizer_and_SVD(train_merged)\n",
    "    trainHistories = vectorize_histories(train_histories, vectorizer, svd)\n",
    "    trainTweets = vectorize_tweets(train_labeled, vectorizer, svd)\n",
    "    testHistories = vectorize_histories(test_histories, vectorizer, svd)\n",
    "    testTweets = vectorize_tweets(test_labeled, vectorizer, svd)\n",
    "    \n",
    "    # tfidf sequence features\n",
    "    X_seq_tfidf_train = proccess_seq_tfidf(train_histories_by_target, vectorizer, svd)\n",
    "    X_seq_tfidf_test = proccess_seq_tfidf(test_histories_by_target, vectorizer, svd)\n",
    "    \n",
    "    \n",
    "    # save data\n",
    "    np.save(out_dir + 'y_train.npy', y_train)\n",
    "    np.save(out_dir + 'y_test.npy', y_test)\n",
    "    \n",
    "    np.save(out_dir + 'X_labeled_train.npy', X_labeled_train)\n",
    "    np.save(out_dir + 'X_histories_train.npy', X_histories_train)\n",
    "    np.save(out_dir + 'X_labeled_test.npy', X_labeled_test)\n",
    "    np.save(out_dir + 'X_histories_test.npy', X_histories_test)\n",
    "\n",
    "    np.savez(out_dir + 'X_seq_glove_train.npz', *X_seq_train)\n",
    "    np.savez(out_dir + 'X_seq_glove_test.npz', *X_seq_test)\n",
    "    \n",
    "    np.save(out_dir + 'trainHistories.npy', trainHistories)\n",
    "    np.save(out_dir + 'trainTweets.npy', trainTweets)\n",
    "    np.save(out_dir + 'testHistories.npy', testHistories)\n",
    "    np.save(out_dir + 'testTweets.npy', testTweets)\n",
    "    np.save(out_dir + 'trainClassifications.npy', train_classes)\n",
    "    np.save(out_dir + 'testClassifications.npy', test_classes)\n",
    "    \n",
    "    np.savez(out_dir + 'X_seq_tfidf_train.npz', *X_seq_tfidf_train)\n",
    "    np.savez(out_dir + 'X_seq_tfidf_test.npz', *X_seq_tfidf_test)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
