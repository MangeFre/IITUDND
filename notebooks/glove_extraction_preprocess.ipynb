{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from utilities import data_handler\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer # for tokenization only"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# THE CELL BELLOW CONTAINS THE FILENAMES TO CHANGE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "UNLABLED_DATA = '/Users/ianmagnusson/IITUDND/data/retrieved_data/tweets/mexico_earthquake_extras.json'\n",
    "LABLED_DATA = '/Users/ianmagnusson/IITUDND/data/CrisisMMD_v1.0/json/mexico_earthquake_final_data.json'\n",
    "CLASS_DATA = '/Users/ianmagnusson/IITUDND/data/CrisisMMD_v1.0/annotations/mexico_earthquake_final_data.tsv'\n",
    "NPY_OUTPUT_FILE = '/Users/ianmagnusson/IITUDND/data/extracted_features/glove/preprocessed/mexico.npz'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up glove"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "EMBED_DIM = 200\n",
    "conversion_file = '../models/gensim_glove.txt'\n",
    "# convert glove format to work with gensim. tutorial here https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
    "# _ = glove2word2vec('/Users/ianmagnusson/IITUDND/models/glove.twitter.27B.200d.txt', conversion_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# load model, NOTE this is very slow!\n",
    "glove = KeyedVectors.load_word2vec_format(conversion_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extract glove embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "data = data_handler.DataHandler(UNLABLED_DATA,LABLED_DATA,CLASS_DATA)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def create_id_2_embed(tweet_jsons):\n",
    "    id_2_embed = {}\n",
    "    tokenizer = CountVectorizer().build_tokenizer()\n",
    "    for i, tweet_json in enumerate(tweet_jsons):\n",
    "        embedded = np.zeros(EMBED_DIM)\n",
    "        text = tweet_json['text'].lower()\n",
    "        tokens = [token for token in tokenizer(text) if token not in ENGLISH_STOP_WORDS]\n",
    "        num_in_vocab = 0\n",
    "        for token in tokens:\n",
    "            if token in glove:\n",
    "                embedded += glove[token]\n",
    "                num_in_vocab += 1\n",
    "        if str(tweet_json['id']) in id_2_embed:\n",
    "            print('oops')\n",
    "        if num_in_vocab:\n",
    "            embedded = embedded / num_in_vocab\n",
    "        id_2_embed[str(tweet_json['id'])] = embedded\n",
    "        \n",
    "    return id_2_embed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "id_2_embed = create_id_2_embed(data.merged)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "3521"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 87
    }
   ],
   "source": [
    "len(id_2_embed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "# save as a dictionary {filename : np.array}\n",
    "\n",
    "with open(NPY_OUTPUT_FILE, 'wb+') as fout:\n",
    "    np.savez_compressed(fout, **id_2_embed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}