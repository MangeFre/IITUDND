{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from utilities import data_handler_old as data_handler\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer # for tokenization only\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# THE CELL BELOW CONTAINS THE FILENAMES TO CHANGE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "IMAGE_FEAT_DIR = '/Users/ianmagnusson/IITUDND/data/extracted_features/resnet/maria/'\n",
    "UNLABLED_DATA = '/Users/ianmagnusson/IITUDND/data/retrieved_data/tweets/maria_extras.json'\n",
    "LABLED_DATA = '/Users/ianmagnusson/IITUDND/data/CrisisMMD_v1.0/json/hurricane_maria_final_data.json'\n",
    "CLASS_DATA = '/Users/ianmagnusson/IITUDND/data/CrisisMMD_v1.0/annotations/hurricane_maria_final_data.tsv'\n",
    "NPY_OUTPUT_DIR = '/Users/ianmagnusson/IITUDND/data/extracted_features/combined_ML/maria/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# get img data; only enters overlapping img once\n",
    "labeled_npz = np.load(IMAGE_FEAT_DIR + 'labeled.npz')\n",
    "unlabeled_npz = np.load(IMAGE_FEAT_DIR + 'unlabeled.npz')\n",
    "\n",
    "image_features = {}\n",
    "for file in labeled_npz.files:\n",
    "    image_features[file] = labeled_npz[file]\n",
    "for file in unlabeled_npz.files:\n",
    "    parse = file.split('_')\n",
    "    file_edit = parse[0] + '_' + parse[2] # cut out username\n",
    "    image_features[file_edit] = unlabeled_npz[file]\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#len(labeled_npz.files) + len(unlabeled_npz.files)\n",
    "len(image_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# get tweets from file and split into test/train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data = data_handler.DataHandler(UNLABLED_DATA,LABLED_DATA,CLASS_DATA)\n",
    "\n",
    "train_labeled, train_histories, train_histories_by_target, test_labeled, test_histories, test_histories_by_target, train_merged, train_classes, test_classes = data.get_train_test_split()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up glove"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "conversion_file = '../models/gensim_glove.txt'\n",
    "# convert glove format to work with gensim. tutorial here https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
    "# _ = glove2word2vec('/Users/ianmagnusson/IITUDND/models/glove.twitter.27B.200d.txt', conversion_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# load model, NOTE this is very slow!\n",
    "glove = KeyedVectors.load_word2vec_format(conversion_file)\n",
    "#glove = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extract features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Class labels\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# build class labels\n",
    "\n",
    "y_train = np.array(train_classes)\n",
    "y_test = np.array(test_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# checkpoint!\n",
    "\n",
    "np.save(NPY_OUTPUT_DIR + 'y_train.npy', y_train)\n",
    "np.save(NPY_OUTPUT_DIR + 'y_test.npy', y_test)\n",
    "\n",
    "\n",
    "#y_train = np.load(NPY_OUTPUT_DIR + 'y_train.npy')\n",
    "#y_test = np.load(NPY_OUTPUT_DIR + 'y_test.npy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GLOVE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "EMBED_DIM = 200\n",
    "\n",
    "def embed_tweets(tweet_jsons):\n",
    "    X_embedded = np.zeros((len(tweet_jsons),EMBED_DIM))\n",
    "    tokenizer = CountVectorizer().build_tokenizer()\n",
    "    for i, tweet_json in enumerate(tweet_jsons):\n",
    "        text = tweet_json['text'].lower()\n",
    "        tokens = [token for token in tokenizer(text) if token not in ENGLISH_STOP_WORDS]\n",
    "        num_in_vocab = 0\n",
    "        for token in tokens:\n",
    "            if token in glove:\n",
    "                X_embedded[i] += glove[token]\n",
    "                num_in_vocab += 1\n",
    "        X_embedded[i] = X_embedded[i] / num_in_vocab\n",
    "    return X_embedded\n",
    "\n",
    "def embed_histories(histories):\n",
    "    X_embedded = np.zeros((len(histories),EMBED_DIM))\n",
    "    tokenizer = CountVectorizer().build_tokenizer()\n",
    "    for i, history in enumerate(histories):\n",
    "        text = ' '.join([tweet_json['text'].lower() for tweet_json in history])\n",
    "        tokens = [token for token in tokenizer(text) if token not in ENGLISH_STOP_WORDS]\n",
    "        num_in_vocab = 0\n",
    "        for token in tokens:\n",
    "            if token in glove:\n",
    "                X_embedded[i] += glove[token]\n",
    "                num_in_vocab += 1\n",
    "        X_embedded[i] = X_embedded[i] / num_in_vocab\n",
    "    return X_embedded\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# baseline features\n",
    "\n",
    "X_labeled_train = embed_tweets(train_labeled)\n",
    "X_histories_train = embed_histories(train_histories)\n",
    "X_labeled_test = embed_tweets(test_labeled)\n",
    "X_histories_test = embed_histories(test_histories)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# checkpoint!\n",
    "np.save(NPY_OUTPUT_DIR + 'X_labeled_train.npy', X_labeled_train)\n",
    "np.save(NPY_OUTPUT_DIR + 'X_histories_train.npy', X_histories_train)\n",
    "np.save(NPY_OUTPUT_DIR + 'X_labeled_test.npy', X_labeled_test)\n",
    "np.save(NPY_OUTPUT_DIR + 'X_histories_test.npy', X_histories_test)\n",
    "\n",
    "\n",
    "\n",
    "#X_labeled_train = np.load(NPY_OUTPUT_DIR + 'X_labeled_train.npy')\n",
    "#X_histories_train = np.load(NPY_OUTPUT_DIR + 'X_histories_train.npy')\n",
    "#X_labeled_test = np.load(NPY_OUTPUT_DIR + 'X_labeled_test.npy')\n",
    "#X_histories_test = np.load(NPY_OUTPUT_DIR + 'X_histories_test.npy')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def proccess_seq(histories_by_target):\n",
    "    X_seq = [] # a list of 2d tensors of shape (len(seq), embed_dim)\n",
    "    for history in histories_by_target: # One specific tweet history\n",
    "        X_seq.append(embed_tweets(history))\n",
    "    return X_seq\n",
    "\n",
    "X_seq_train = proccess_seq(train_histories_by_target)\n",
    "X_seq_test = proccess_seq(test_histories_by_target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# checkpoint!\n",
    "np.savez(NPY_OUTPUT_DIR + 'X_seq_glove_train.npz', *X_seq_train)\n",
    "np.savez(NPY_OUTPUT_DIR + 'X_seq_glove_test.npz', *X_seq_test)\n",
    "\n",
    "\n",
    "\n",
    "#X_seq_glove_train = np.load(NPY_INPUT_DIR + 'X_seq_glove_train.npz')\n",
    "#X_seq_glove_test = np.load(NPY_INPUT_DIR + 'X_seq_glove_test.npz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TF-IDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "SVD_COMPONENTS = 200\n",
    "\n",
    "def construct_vectorizer_and_SVD(merged):\n",
    "    allTweets = []\n",
    "    for i, tweet_json in enumerate(merged):\n",
    "        text = tweet_json['text'].lower()\n",
    "        allTweets.append(text)\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_merged = vectorizer.fit_transform(allTweets)\n",
    "    svd = TruncatedSVD(n_components=SVD_COMPONENTS, n_iter=7, random_state=42)\n",
    "    svd.fit(tfidf_merged)    \n",
    "    return vectorizer, svd\n",
    "\n",
    "def vectorize_histories(histories, vectorizer, svd):\n",
    "    rawHistories = [] # Will be in order\n",
    "    for i, history in enumerate(histories):\n",
    "        text = ' '.join([tweet_json['text'].lower() for tweet_json in history])\n",
    "        rawHistories.append(text)\n",
    "        \n",
    "        # loop thru all tweets in hist, all img in tweet and add to vector and count and div\n",
    "    histArr = vectorizer.transform(rawHistories)\n",
    "    histFeatureArr = svd.transform(histArr)\n",
    "    return histFeatureArr\n",
    "\n",
    "\n",
    "def vectorize_tweets(tweets, vectorizer, svd):\n",
    "    labeledTweets = []  # Will be in order\n",
    "    for i, tweet_json in enumerate(tweets):\n",
    "        text = tweet_json['text'].lower()\n",
    "        labeledTweets.append(text)\n",
    "    tweetArr = vectorizer.transform(labeledTweets)\n",
    "    tweetFeatureArr = svd.transform(tweetArr)\n",
    "    return tweetFeatureArr    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Images"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "IMG_SVD_COMPONENTS = 400 \n",
    "def construct_image_svd(image_features, train_merged):\n",
    "    '''\n",
    "    Fits image SVD on all images from train_merged\n",
    "    '''\n",
    "    image_features_error = 0\n",
    "    lookup_error = 0\n",
    "    X_img_seq = [] # create n images by (SVD length) matrix - stores one vector per image\n",
    "    for tweet in train_merged: # specific tweet history for each item in merged\n",
    "        if 'extended_entities' in tweet.keys():\n",
    "            for i in range(len(tweet['extended_entities']['media'])):\n",
    "                imageID = (str(tweet['id']) +'_'+str(i)+'.jpg')\n",
    "                try:\n",
    "                    # Extract the image ID for each image, and look up the image_features corresponding vec\n",
    "                    X_img_seq.append(image_features[imageID])\n",
    "                except KeyError:\n",
    "                    image_features_error += 1          \n",
    "        else:\n",
    "            lookup_error += 1\n",
    "    print(X_img_seq[0])\n",
    "    print(image_features_error, \"cant-find-vector errors and \", lookup_error, \"lookup errors for extended entities\")\n",
    "    X_img_seq = np.stack(X_img_seq, axis=0)\n",
    "    print(\"Shape of SVD input:\", X_img_seq.shape)\n",
    "    svd = TruncatedSVD(n_components=IMG_SVD_COMPONENTS, n_iter=7, random_state=42)\n",
    "    svd.fit(X_img_seq)\n",
    "    return svd\n",
    "\n",
    "def vectorize_labeledimgs(ImgSVD, train_labeled, test_labeled):\n",
    "    train_svds = []\n",
    "    test_svds = []\n",
    "    noKeyMatch = 0\n",
    "    noEntities = 0\n",
    "    for tweet in train_labeled:\n",
    "        images = [] # Iterate through each image in each labeled tweet\n",
    "        if 'extended_entities' in tweet.keys():\n",
    "            count = 0\n",
    "            for j in range(len(tweet['extended_entities']['media'])):\n",
    "                try:\n",
    "                    images.append(image_features[str(tweet['id'])+'_'+str(count)+'.jpg'])\n",
    "                    count += 1\n",
    "                except KeyError:\n",
    "                    noKeyMatch += 1\n",
    "            # if images were found, aggregate into matrix and SVD it\n",
    "            if count > 0:\n",
    "                images = np.stack(images, axis=0) # stack img vectors into matrix\n",
    "                images = ImgSVD.transform(images) # transform img vectors\n",
    "                train_svds.append(images) # add img matrix to the target's hist list\n",
    "            else:\n",
    "                train_svds.append([0] * 400) # otherwise, append an empty matrix to make sure order\n",
    "                # is preserved\n",
    "        else:\n",
    "            noEntities += 1\n",
    "            train_svds.append([0] * 400)\n",
    "    print(\"SVDs composed for labeled train tweets with\", noEntities, \"cases without images and\",\n",
    "          noKeyMatch, \"failed image matches\")\n",
    "    noKeyMatch = 0\n",
    "    noEntities = 0\n",
    "    for tweet in test_labeled:\n",
    "        images = [] # start list of images in the historic tweet\n",
    "        if 'extended_entities' in tweet.keys():\n",
    "            count = 0\n",
    "            for j in range(len(tweet['extended_entities']['media'])):\n",
    "                try:\n",
    "                    images.append(image_features[str(tweet['id'])+'_'+str(count)+'.jpg'])\n",
    "                    count += 1\n",
    "                except KeyError:\n",
    "                    noKeyMatch += 1\n",
    "            # If images were found, aggregate into SVD and append it\n",
    "            if count > 0:\n",
    "                images = np.stack(images, axis=0) # stack img vectors into matrix\n",
    "                images = ImgSVD.transform(images) # transform img vectors\n",
    "                test_svds.append(images) # add img matrix to the target's hist list\n",
    "            else:\n",
    "                test_svds.append([0] * 400) # append a null value to make sure values still align\n",
    "        else:\n",
    "            noEntities += 1\n",
    "            test_svds.append([0] * 400)\n",
    "    print(\"SVDs composed for labeled test tweets with\", noEntities, \"cases without images and\",\n",
    "          noKeyMatch, \"failed image matches\")\n",
    "    return train_svds, test_svds # Each returned array should be of same length as train/test.\n",
    "\n",
    "def vectorize_images(image_features, histories_by_target, ImgSVD):\n",
    "    lstmHistories = [] # first output: for the LSTMs\n",
    "    nMatrices = [] # second output: list of matrices of all images for each of n tweets\n",
    "    nVectors = [] # third output: matrix of average img vector for each of n tweets\n",
    "    noEntities = 0 \n",
    "    noKeyMatch = 0\n",
    "    for target in histories_by_target: # specific tweet history from keys:TweetID, vals=vectors\n",
    "        allTargetTweets = []\n",
    "        historyTweets = []\n",
    "        for historicTweet in target: # iterate through each tweet in history\n",
    "            historicTweetImages = [] # start list of images in the historic tweet\n",
    "            if 'extended_entities' in historicTweet.keys():\n",
    "                count = 0\n",
    "                for j in range(len(historicTweet['extended_entities']['media'])):\n",
    "                    try:\n",
    "                        # locate the feature vector for LSTM output\n",
    "                        historicTweetImages.append(image_features[str(historicTweet['id'])+'_'+str(count)+'.jpg'])\n",
    "                        # for MLP output\n",
    "                        allTargetTweets.append(image_features[str(historicTweet['id'])+'_'+str(count)+'.jpg'])\n",
    "                        count += 1\n",
    "                    except KeyError:\n",
    "                        noKeyMatch += 1\n",
    "                # if historic tweets were found, aggregate into matrix and SVD it\n",
    "                if count > 0:\n",
    "                    historicTweetImages = np.stack(historicTweetImages, axis=0) # stack img vectors into matrix\n",
    "                    historicTweetImages = ImgSVD.transform(historicTweetImages) # transform img vectors\n",
    "                    historyTweets.append(historicTweetImages) # add img matrix to the target's hist list\n",
    "                else:\n",
    "                    zeroArray = np.zeros((1,400))\n",
    "                    historyTweets.append(zeroArray)\n",
    "            else:\n",
    "                noEntities += 1\n",
    "                zeroArray = np.zeros((1,400))\n",
    "                historyTweets.append(zeroArray)\n",
    "        if len(allTargetTweets) > 0: # see how many images you collected\n",
    "            # extract mean img vector for that target\n",
    "            nVectors.append(np.mean(allTargetTweets, axis=0))\n",
    "            allTargetTweets = np.stack(allTargetTweets, axis=0)\n",
    "            allTargetTweets = ImgSVD.transform(allTargetTweets)\n",
    "            nMatrices.append(allTargetTweets)\n",
    "        else:\n",
    "            zeroArray = np.zeros((1,400))\n",
    "            nMatrices.append(zeroArray)\n",
    "            nVectors.append([0] * 2048)\n",
    "        if len(historyTweets) > 0:\n",
    "            lstmHistories.append(historyTweets)\n",
    "        else:\n",
    "            zeroArray = np.zeros((1,400))\n",
    "            lstmHistories.append(zeroArray)\n",
    "        \n",
    "    # stack up the mean image vectors\n",
    "    nVectors = np.stack(nVectors, axis=0)\n",
    "    nVectors = ImgSVD.transform(nVectors)\n",
    "    return lstmHistories, nMatrices, nVectors\n",
    "    # lstmHistories contains output for LSTM \n",
    "    # nMatrices contains: list of n matrices of (#imgs, 200) for each tweet in history\n",
    "    # nVectors: is one matrix with one row per target representing mean image vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%capture\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# BASELINE\n",
    "# assuming train_histories is a list of history IDs for each target\n",
    "\n",
    "vectorizer, svd = construct_vectorizer_and_SVD(train_merged)\n",
    "trainHistories = vectorize_histories(train_histories, vectorizer, svd)\n",
    "trainTweets = vectorize_tweets(train_labeled, vectorizer, svd)\n",
    "testHistories = vectorize_histories(test_histories, vectorizer, svd)\n",
    "testTweets = vectorize_tweets(test_labeled, vectorizer, svd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Save to outfiles\n",
    "np.save(NPY_OUTPUT_DIR + 'trainHistories.npy', trainHistories)\n",
    "np.save(NPY_OUTPUT_DIR + 'trainTweets.npy', trainTweets)\n",
    "np.save(NPY_OUTPUT_DIR + 'testHistories.npy', testHistories)\n",
    "np.save(NPY_OUTPUT_DIR + 'testTweets.npy', testTweets)\n",
    "np.save(NPY_OUTPUT_DIR + 'trainClassifications.npy', train_classes)\n",
    "np.save(NPY_OUTPUT_DIR + 'testClassifications.npy', test_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[0.24662529 0.5358104  0.06267021 ... 0.70655394 0.25315675 0.00836127]\n",
      "351 cant-find-vector errors and  12228 lookup errors for extended entities\n",
      "Shape of SVD input: (7854, 2048)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# New image methods\n",
    "imageSVD = construct_image_svd(image_features, train_merged) # fit SVD on merged\n",
    "trainMatrix_LSTM, trainMatrix_AllImgsMLP, trainMatrix_MeanImgMLP = vectorize_images(image_features, train_histories, imageSVD)  # Get image vectors for train\n",
    "testMatrix_LSTM, testMatrix_AllImgsMLP, testMatrix_MeanImgMLP = vectorize_images(image_features, test_histories, imageSVD) # Get image vectors for test\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Shape of sample train images matrix: (1, 400)\n",
      "Shape of sample test images matrix: (1, 400)\n",
      "Length of list of MLP matrices: 3199\n",
      "Shape of matrix of MLP means: (3199, 400)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Validation checks\n",
    "# print(\"Shape of train history np array:\", trainHistories.shape)\n",
    "# print(\"Shape of train tweets np array):\", trainTweets.shape)\n",
    "# print(\"Shape of test history np array:\", testHistories.shape)\n",
    "# print(\"Shape of test tweets np array:\", testTweets.shape)\n",
    "print(\"Shape of sample train images matrix:\", trainMatrix_LSTM[0][0].shape)\n",
    "print(\"Shape of sample test images matrix:\", testMatrix_LSTM[0][0].shape)\n",
    "print(\"Length of list of MLP matrices:\", len(trainMatrix_AllImgsMLP))\n",
    "print(\"Shape of matrix of MLP means:\", trainMatrix_MeanImgMLP.shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# sequence features\n",
    "\n",
    "def proccess_seq_tfidf(histories_by_target, vectorizer, svd):\n",
    "    X_seq = [] # a list of 2d tensors of shape (len(seq), SVD_COMPONENTS)\n",
    "    for history in histories_by_target:\n",
    "        X_seq.append(vectorize_tweets(history, vectorizer, svd))\n",
    "    return X_seq\n",
    "\n",
    "X_seq_tfidf_train = proccess_seq_tfidf(train_histories_by_target, vectorizer, svd)\n",
    "X_seq_tfidf_test = proccess_seq_tfidf(test_histories_by_target, vectorizer, svd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "#TODO: Follow this format of saving matrices\n",
    "\n",
    "# checkpoint!\n",
    "np.savez(NPY_OUTPUT_DIR + 'X_seq_tfidf_train.npz', *X_seq_tfidf_train)\n",
    "np.savez(NPY_OUTPUT_DIR + 'X_seq_tfidf_test.npz', *X_seq_tfidf_test)\n",
    "np.savez(NPY_OUTPUT_DIR + 'images_lstm_train.npz', *trainMatrix_LSTM)\n",
    "np.savez(NPY_OUTPUT_DIR + 'images_lstm_test.npz', *testMatrix_LSTM)\n",
    "np.save(NPY_OUTPUT_DIR + 'images_matrixlists_train.npy', trainMatrix_AllImgsMLP)\n",
    "np.save(NPY_OUTPUT_DIR + 'images_matrixlists_test.npy', testMatrix_AllImgsMLP)\n",
    "np.savez(NPY_OUTPUT_DIR + 'images_meanvecs_train.npz', *trainMatrix_MeanImgMLP)\n",
    "np.savez(NPY_OUTPUT_DIR + 'images_meanvecs_test.npz', *testMatrix_MeanImgMLP)\n",
    "#X_seq_tfidf_train = np.load(NPY_OUTPUT_DIR + 'X_seq_tfidf_train.npz')\n",
    "#X_seq_tfidf_test = np.load(NPY_OUTPUT_DIR + 'X_seq_tfidf_test.npz')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}