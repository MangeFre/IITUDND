{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tweepy\n",
    "# We must use the API to determine whether the tweets are protected\n",
    "from tweepy import TweepError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"PQSsqSxGft69D53XGknXya8cL\"\n",
    "consumer_secret = \"dlh6EFLADanVNvdl9Q9Lz9Iv3yQQyHY9vxKTS9BN2skflGd45d\"\n",
    "access_token = \"1187382763432771584-rwZbZc9lcOcaVdgxcIgqqCPqoQMo3I\"\n",
    "access_token_secret = \"nvnIFiZD9L1u6tZwntl24BtsLhKmkBntJ4dpko82fYKg8\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmdUserHistories = defaultdict(list)\n",
    "tweetAuthors = defaultdict(str)\n",
    "mmdTweetsToText = defaultdict(str)\n",
    "allTweetsToText = defaultdict(str)\n",
    "\n",
    "# Read in the JSON data to get the text of each tweet and build user histories\n",
    "#TODO: Change the filepath here\n",
    "with open(\"/Users/chloelarkin/Desktop/hurricane_harvey_final_data.json\") as fin:\n",
    "    for line in fin:\n",
    "        json_data = json.loads(line)\n",
    "        user = json_data['user']['id'] # Get the user ID\n",
    "        tweet_id = json_data['id']\n",
    "        tweet_text = json_data['text']  # Get the text of the tweet\n",
    "        tweetAuthors[tweet_id] = user\n",
    "        mmdTweetsToText[tweet_id] = tweet_text\n",
    "        allTweetsToText[tweet_id] = tweet_text\n",
    "        mmdUserHistories[user].append(tweet_id)\n",
    "        # try: #TODO: This is where we can filter out private tweets\n",
    "        #     u = api.get_user(user) # Check if the user's tweets are protected. If they are public, then append to our dataset\n",
    "        #     if not u.protected:\n",
    "        #         tweet_id = json_data['id']\n",
    "        #         userTweetDict[tweet_id] = user\n",
    "        #         tweet_text = json_data['text'] # Get the text of the tweet\n",
    "        #         originalTweetText[user].append(tweet_text)\n",
    "        #         originalTweetIDs[user].append(tweet_id)\n",
    "        # except TweepError as e:\n",
    "        #     if e.args[0][0]['code'] == 63: # The error code for suspended users. Skip these people\n",
    "        #         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, build up a history for each user from the extra tweet corpus\n",
    "extraTweets = defaultdict(list)\n",
    "with open(\"/Users/chloelarkin/Desktop/iraq_earthquake_extras.json\") as fin:\n",
    "    for line in fin:\n",
    "        json_data = json.loads(line)\n",
    "        user = json_data['user']['id'] # Get the user ID\n",
    "        if user in mmdUserHistories.keys(): # Check whether the user ID matches someone in the CrisisMMD group\n",
    "            tweet_id = json_data['id']\n",
    "            if tweet_id not in mmdUserHistories[user]:  # Check to make sure you're not duplicating a tweet\n",
    "                mmdUserHistories[user].append(tweet_id) # Add the current tweet to mmdUserHistories\n",
    "                tweet_text = json_data['text']  # Get the text of the tweet\n",
    "                tweetAuthors[tweet_id] = user\n",
    "                allTweetsToText[tweet_id] = tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each tweet, store list of full user history tweet IDs\n",
    "fullHistoryPerTweet = defaultdict(list)\n",
    "for user in mmdUserHistories:\n",
    "    for tweet in mmdUserHistories[user].copy():\n",
    "        fullHistoryPerTweet[tweet] = mmdUserHistories[user].copy()\n",
    "\n",
    "# For each tweet, make a document out of full history\n",
    "fullHistoryDocs = defaultdict(str)\n",
    "for tweet in fullHistoryPerTweet:\n",
    "    fullHistoryDocs[tweet] = ' '.join(allTweetsToText[t] for t in fullHistoryPerTweet[tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted list of authors for each tweet in the full corpus\n",
    "authors = [tweetAuthors[v] for v in sorted(tweetAuthors.keys())]\n",
    "\n",
    "# Sorted list of tweet strings for each tweet in the full corpus\n",
    "allTweetsText = [allTweetsToText[v] for v in sorted(allTweetsToText.keys())]\n",
    "\n",
    "# Sorted list of tweet strings for JUST the labeled set\n",
    "mmdTweetsText = [mmdTweetsToText[v] for v in sorted(mmdTweetsToText.keys())]\n",
    "\n",
    "# Sorted list of user histories for JUST the labeled set\n",
    "mmdTweetsHistories = [fullHistoryDocs[v] for v in sorted(fullHistoryDocs.keys()) if v in mmdTweetsToText.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish TFIDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on ALL LABELED AND UNLABELED TWEETS = the corpus\n",
    "vectorizer.fit(allTweetsText)\n",
    "\n",
    "# Get TFIDF vectors for each labeled tweet\n",
    "tweetVecs = vectorizer.transform(mmdTweetsText)\n",
    "\n",
    "# Get TFIDF vectors for the user histories of every labeled tweet\n",
    "userHistoryVecs = vectorizer.transform(mmdTweetsHistories)\n",
    "\n",
    "# Validation check of number of tweets:\n",
    "print(\"There were\", len(mmdTweetsToText), \"original crisisMMD tweets, and there are\", len(allTweetsToText),\n",
    "      \"tweets including the extras we extracted.\")\n",
    "\n",
    "# Validation check of relative dimensions of both matrices:\n",
    "print(\"Dimensions of labeled tweet vectors:\", tweetVecs.get_shape(),\n",
    "      \"and dimensions of user histories for each labeled tweet:\", userHistoryVecs.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SVD dimension reduction on both matrices (separately):\n",
    "# SVD for individual labeled tweets:\n",
    "individualSVD = TruncatedSVD(n_components=200, n_iter=7, random_state=42)\n",
    "tweetFeatureArr = individualSVD.fit_transform(tweetVecs)\n",
    "tweetFeatureArr = np.array(tweetFeatureArr)\n",
    "print(\"Shape of individual tweet feature np array:\", tweetFeatureArr.shape)\n",
    "\n",
    "# SVD for user histories:\n",
    "historySVD = TruncatedSVD(n_components=200, n_iter=7, random_state=42)\n",
    "histFeatureArr = historySVD.fit_transform(userHistoryVecs)\n",
    "histFeatureArr = np.array(histFeatureArr)\n",
    "print(\"Shape of history feature np array:\", histFeatureArr.shape)\n",
    "\n",
    "# At the end we have tweetFeatureArr and histFeatureArr each of which store the results from SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART TWO: Get the classification numpy array\n",
    "\n",
    "# Establish dicts of annotated scores for each tweet ID\n",
    "text_informative = defaultdict(str) #4000 scores -- this is the binary informative / not informative score\n",
    "text_humanitarian = defaultdict(str) # 4000 scores -- finer-grain classification\n",
    "\n",
    "# Read in the classifications of each tweet\n",
    "with open(\"/Users/chloelarkin/Desktop/iraq_iran_earthquake_final_data.tsv\") as fin:\n",
    "    reader = csv.DictReader(fin, dialect='excel-tab')\n",
    "    for line in reader:\n",
    "        text_informative[line['tweet_id']]=line['text_info']\n",
    "        text_humanitarian[line['tweet_id']]=line['text_human']\n",
    "\n",
    "# Retrieve scores for each tweet in the dataset\n",
    "informative_scores = defaultdict(str)\n",
    "humanitarian_scores = defaultdict(str)\n",
    "for id in mmdTweetsToText.keys():\n",
    "    informative_scores[id] = text_informative[\"\\\"\"+ str(id) + \"\\\"\"]\n",
    "for id in mmdTweetsToText.keys():\n",
    "    humanitarian_scores[id] = text_humanitarian[\"\\\"\"+ str(id) + \"\\\"\"]\n",
    "\n",
    "informativeScoreArr = [informative_scores[v] for v in sorted(informative_scores.keys())]\n",
    "informativeScoreArr = np.array(informativeScoreArr)\n",
    "print(\"Informativeness classification np array shape:\", informativeScoreArr.shape)\n",
    "\n",
    "humanitarianScoreArr = [humanitarian_scores[v] for v in sorted(humanitarian_scores.keys())]\n",
    "humanitarianScoreArr = np.array(humanitarianScoreArr)\n",
    "print(\"Humanitarian classification np array shape:\", humanitarianScoreArr.shape)\n",
    "\n",
    "# Sanity check to make sure there is one score for each tweet in the CrisisMMD set (should be 499)\n",
    "print(\"There are\", len(mmdTweetsToText), \"MMD tweets and\", len(informative_scores),\n",
    "      \"informative scores and\", len(humanitarian_scores), \"humanitarian scores.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
