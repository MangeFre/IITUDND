{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import csv\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tweepy\n",
    "# We must use the API to determine whether the tweets are protected\n",
    "from tweepy import TweepError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"PQSsqSxGft69D53XGknXya8cL\"\n",
    "consumer_secret = \"dlh6EFLADanVNvdl9Q9Lz9Iv3yQQyHY9vxKTS9BN2skflGd45d\"\n",
    "access_token = \"1187382763432771584-rwZbZc9lcOcaVdgxcIgqqCPqoQMo3I\"\n",
    "access_token_secret = \"nvnIFiZD9L1u6tZwntl24BtsLhKmkBntJ4dpko82fYKg8\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a test run for just the Iraq earthquake event (it is smaller and easier to work with)\n",
    "'''\n",
    "originalUserHistories = defaultdict(list)\n",
    "userTweetDict = defaultdict(str)\n",
    "tweetText = defaultdict(str)\n",
    "origTweets = defaultdict(str)\n",
    "\n",
    "# Read in the JSON data to get the text of each tweet and build user histories\n",
    "#TODO: Change the filepath here\n",
    "with open(\"/Users/chloelarkin/Desktop/iraq_earthquake_final_data.json\") as fin:\n",
    "    for line in fin:\n",
    "        json_data = json.loads(line)\n",
    "        user = json_data['user']['id'] # Get the user ID\n",
    "        tweet_id = json_data['id']\n",
    "        origTweets[tweet_id] = user\n",
    "        userTweetDict[tweet_id] = user\n",
    "        tweet_text = json_data['text']  # Get the text of the tweet\n",
    "        tweetText[tweet_id] = tweet_text\n",
    "        originalUserHistories[user].append(tweet_id)\n",
    "        # try: #TODO: This is where we can filter out private tweets\n",
    "        #     u = api.get_user(user) # Check if the user's tweets are protected. If they are public, then append to our dataset\n",
    "        #     if not u.protected:\n",
    "        #         tweet_id = json_data['id']\n",
    "        #         userTweetDict[tweet_id] = user\n",
    "        #         tweet_text = json_data['text'] # Get the text of the tweet\n",
    "        #         originalTweetText[user].append(tweet_text)\n",
    "        #         originalTweetIDs[user].append(tweet_id)\n",
    "        # except TweepError as e:\n",
    "        #     if e.args[0][0]['code'] == 63: # The error code for suspended users. Skip these people\n",
    "        #         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tweet histories from original dataset if a user has multiple tweets in the original dataset\n",
    "indTweetsHistories = defaultdict(list) # This stores the tweet IDs of other tweets from that user in the original dataset\n",
    "for user in originalUserHistories:\n",
    "    if len(originalUserHistories[user]) > 1: # Check that the user has written more than one tweet\n",
    "        for tweet in originalUserHistories[user].copy():\n",
    "            copyExtraTweets = originalUserHistories[user].copy()\n",
    "            copyExtraTweets.pop(copyExtraTweets.index(tweet))\n",
    "            indTweetsHistories[tweet] = copyExtraTweets\n",
    "\n",
    "# Next, build up a history for each user from the extra tweet corpus\n",
    "extraTweets = defaultdict(list)\n",
    "with open(\"/Users/chloelarkin/Desktop/iraq_earthquake_extras.json\") as fin:\n",
    "    for line in fin:\n",
    "        json_data = json.loads(line)\n",
    "        user = json_data['user']['id'] # Get the user ID\n",
    "        if user in originalUserHistories: # Only collect tweets from users in the MMD dataset\n",
    "            tweet_id = json_data['id']\n",
    "            tweet_text = json_data['text'] # Get the text of the tweet\n",
    "            tweetText[tweet_id] = tweet_text # Continue building dict from tweet ID to string text\n",
    "            extraTweets[user].append(tweet_id) # This appends the tweet ID to the extraTweets[user] list\n",
    "\n",
    "# Get rid of any duplicates between the original dataset and the extra dataset\n",
    "for tweet in extraTweets[user]:\n",
    "    if tweet in origTweets.keys():\n",
    "        extraTweets[user].remove(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tweetHistory dict storing the user's history for each labeled tweet (exclusive of the current labeled tweet)\n",
    "historyDocs = defaultdict(str)\n",
    "for tweet in origTweets: # this dict refers to all tweets in the dataset\n",
    "    user = userTweetDict[tweet] # look up which user the tweet belonged to so that you can find the extra tweets\n",
    "    if len(indTweetsHistories) > 0: # Case where there is at least 1 other tweet from the same author in CrisisMMD\n",
    "        if len(extraTweets[user]) > 0:\n",
    "            for newTweet in extraTweets[user]: # Find the extra tweets for that user\n",
    "                indTweetsHistories[tweet].append(newTweet) # Append them to this tweet history\n",
    "        listTweets = []\n",
    "        for entry in indTweetsHistories[tweet]:\n",
    "            listTweets.append(tweetText[entry])\n",
    "        if len(listTweets) > 0:\n",
    "            historyDocs[tweet] = ' '.join(listTweets)\n",
    "    else: # Case where there are no tweets from the same user in CrisisMMD, but there are among the tweets we extracted\n",
    "        if len(extraTweets[user]) > 0:\n",
    "            listTweets = []\n",
    "            for newTweet in extraTweets[user]:\n",
    "                listTweets.append(newTweet)\n",
    "            if len(listTweets) > 0:\n",
    "                historyDocs[tweet] = ' '.join(listTweets)\n",
    "tweets = list(key for key in historyDocs.keys())\n",
    "corpus = list(value for value in historyDocs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TF-IDF vectors for each tweet in the original corpus\n",
    "tfidfVectors = defaultdict(list)  # format:  tfidfVectors[user] = tf-idf arr<>\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Construct TF-IDF vectors for each user history from ALL EXTRA TWEETS OF USERS IN THE ORIGINAL AND NEW DATASET\n",
    "vectors = vectorizer.fit_transform(corpus)\n",
    "for i in range(len(tweets)):\n",
    "    tfidfVectors[i] = vectors[i] # Map vectors to their users\n",
    "print(\"# of TFIDF vectors:\", len(tfidfVectors)) # Test to make sure there are the correct number of vectors\n",
    "\n",
    "print(\"There were\", len(tweetText), \"total tweets,\", len(origTweets),\n",
    "      \"tweets in the CrisisMMD dataset, and \", len(historyDocs), \"tweets with histories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish dicts of annotated scores for each tweet ID\n",
    "text_informative = defaultdict(str) #4000 scores -- this is the binary informative / not informative score\n",
    "text_humanitarian = defaultdict(str) # 4000 scores -- finer-grain classification\n",
    "\n",
    "# Read in the classifications of each tweet\n",
    "with open(\"/Users/chloelarkin/Desktop/iraq_iran_earthquake_final_data.tsv\") as fin:\n",
    "    reader = csv.DictReader(fin, dialect='excel-tab')\n",
    "    for line in reader:\n",
    "        text_informative[line['tweet_id']]=line['text_info']\n",
    "        text_humanitarian[line['tweet_id']]=line['text_human']\n",
    "\n",
    "# Retrieve scores for each tweet in the dataset\n",
    "informative_scores = defaultdict(str)\n",
    "humanitarian_scores = defaultdict(str)\n",
    "for id in origTweets.keys():\n",
    "    informative_scores[id] = text_informative[\"\\\"\"+ str(id) + \"\\\"\"]\n",
    "for id in origTweets.keys():\n",
    "    humanitarian_scores[id] = text_humanitarian[\"\\\"\"+ str(id) + \"\\\"\"]\n",
    "\n",
    "# Sanity check to make sure there is one score for each tweet in the CrisisMMD set\n",
    "print(\"There are\", len(origTweets), \"tweets and\", len(informative_scores),\n",
    "      \"informative scores and\", len(humanitarian_scores), \"humanitarian scores.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
